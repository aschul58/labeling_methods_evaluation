{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "735f1720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from config.ipynb\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import import_ipynb\n",
    "from config import *\n",
    "import pandas as pd\n",
    "import empyrical\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a43b30af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_overall_accuracy(dictionary):\n",
    "    \"\"\"\n",
    "    This function prints the overall accuracy of the trained model on both the training and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    dictionary (dict): A dictionary containing the trained model, training and test datasets, and true labels.\n",
    "\n",
    "    Returns:\n",
    "    None: This function prints the accuracy scores directly.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calculate and print the accuracy score for the training set\n",
    "    train_accuracy = dictionary[\"model\"].score(dictionary[\"X_train\"], dictionary[\"y_train\"])\n",
    "    print('Training set score: {:.4f}'.format(train_accuracy))\n",
    "    \n",
    "    # Calculate and print the accuracy score for the test set\n",
    "    test_accuracy = dictionary[\"model\"].score(dictionary[\"X_test\"], dictionary[\"y_test\"])\n",
    "    print('Test set score: {:.4f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b50dc9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_overall_accuracy(dictionary):\n",
    "    \"\"\"\n",
    "    This function evaluates the overall accuracy of trained models for multiple labeling methods\n",
    "    and stores the results in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    dictionary (dict): A dictionary where keys are labeling method names and values are dictionaries \n",
    "                       containing the trained model, training and test datasets, and true labels.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the accuracy scores for each labeling method.\n",
    "    \"\"\"\n",
    "    results = []  # Initialize an empty list to store results\n",
    "    \n",
    "    # Iterate over each labeling method in the dictionary\n",
    "    for labeling_method in dictionary:\n",
    "        # Evaluate the accuracy for the current method\n",
    "        train_accuracy, test_accuracy = get_overall_accuracy(dictionary[labeling_method])\n",
    "        \n",
    "        # Append the results to the list\n",
    "        results.append({\n",
    "            'Labeling Method': labeling_method,\n",
    "            'Training Set Score': train_accuracy,\n",
    "            'Test Set Score': test_accuracy\n",
    "        })\n",
    "    \n",
    "    # Convert the results list to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a17e67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overall_accuracy(dictionary):\n",
    "    \"\"\"\n",
    "    This function calculates the overall accuracy of the trained model on both the training and test sets.\n",
    "\n",
    "    Parameters:\n",
    "    dictionary (dict): A dictionary containing the trained model, training and test datasets, and true labels.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the training set accuracy and test set accuracy.\n",
    "    \"\"\"\n",
    "    # Calculate the accuracy score for the training set\n",
    "    train_accuracy = dictionary[\"model\"].score(dictionary[\"X_train\"], dictionary[\"y_train\"])\n",
    "    \n",
    "    # Calculate the accuracy score for the test set\n",
    "    test_accuracy = dictionary[\"model\"].score(dictionary[\"X_test\"], dictionary[\"y_test\"])\n",
    "    \n",
    "    return train_accuracy, test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "806c13c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_currency_accuracy(dictionary):\n",
    "    \"\"\"\n",
    "    This function evaluates various ML metrics of trained models for each currency and labeling method,\n",
    "    and stores the results in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    dictionary (dict): A dictionary where keys are labeling method names and values are dictionaries containing\n",
    "                       the trained model information for each currency. Each currency dictionary should contain\n",
    "                       'train' and 'test' keys with true and predicted labels.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the evaluation metrics for each currency and labeling method.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mapping of internal names to real names\n",
    "    labeling_method_names = {\n",
    "        'excess_over_mean': 'Excess over Mean',\n",
    "        'excess_over_median': 'Excess over Median',\n",
    "        'fixed_time_horizon': 'Fixed Time Horizon',\n",
    "        'triple_barrier': 'Triple Barrier',\n",
    "        'tail_sets': 'Tail Sets',\n",
    "        'matrix_flag': 'Matrix Flag',\n",
    "        'trend_scanning': 'Trend Scanning',\n",
    "        'buy_and_hold': 'Buy and Hold',\n",
    "        \"next_period\": 'Next Period Labeling'\n",
    "    }\n",
    "\n",
    "    # Initialize an empty list to store results\n",
    "    results = []\n",
    "\n",
    "    # Iterate over each labeling method in the dictionary\n",
    "    for labeling_method in dictionary:\n",
    "        real_labeling_method = labeling_method_names.get(labeling_method, labeling_method)\n",
    "        # Iterate over each currency in the current labeling method\n",
    "        for currency in dictionary[labeling_method]:\n",
    "            y_train_true = dictionary[labeling_method][currency][\"train\"][\"true label\"]\n",
    "            y_train_pred = dictionary[labeling_method][currency][\"train\"][\"predicted label\"]\n",
    "            y_test_true = dictionary[labeling_method][currency][\"test\"][\"true label\"]\n",
    "            y_test_pred = dictionary[labeling_method][currency][\"test\"][\"predicted label\"]\n",
    "            \n",
    "            # Calculate evaluation metrics for training set\n",
    "            train_accuracy = accuracy_score(y_train_true, y_train_pred)\n",
    "            train_precision = precision_score(y_train_true, y_train_pred, average='weighted', zero_division=0)\n",
    "            train_recall = recall_score(y_train_true, y_train_pred, average='weighted', zero_division=0)\n",
    "            train_f1 = f1_score(y_train_true, y_train_pred, average='weighted', zero_division=0)\n",
    "            train_balanced_acc = balanced_accuracy_score(y_train_true, y_train_pred)\n",
    "            \n",
    "            # Calculate evaluation metrics for test set\n",
    "            test_accuracy = accuracy_score(y_test_true, y_test_pred)\n",
    "            test_precision = precision_score(y_test_true, y_test_pred, average='weighted', zero_division=0)\n",
    "            test_recall = recall_score(y_test_true, y_test_pred, average='weighted', zero_division=0)\n",
    "            test_f1 = f1_score(y_test_true, y_test_pred, average='weighted', zero_division=0)\n",
    "            test_balanced_acc = balanced_accuracy_score(y_test_true, y_test_pred)\n",
    "            \n",
    "            # Append the results to the list for train data\n",
    "            results.append({\n",
    "                'Labeling Method': real_labeling_method,\n",
    "                'Currency': currency,\n",
    "                'Data Type': 'Train',\n",
    "                'Accuracy': train_accuracy,\n",
    "                'Precision': train_precision,\n",
    "                'Recall': train_recall,\n",
    "                'F1 Score': train_f1,\n",
    "                'Balanced Accuracy': train_balanced_acc\n",
    "            })\n",
    "            \n",
    "            # Append the results to the list for test data\n",
    "            results.append({\n",
    "                'Labeling Method': real_labeling_method,\n",
    "                'Currency': currency,\n",
    "                'Data Type': 'Test',\n",
    "                'Accuracy': test_accuracy,\n",
    "                'Precision': test_precision,\n",
    "                'Recall': test_recall,\n",
    "                'F1 Score': test_f1,\n",
    "                'Balanced Accuracy': test_balanced_acc\n",
    "            })\n",
    "\n",
    "    # Convert the results list to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ebb094c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_training_results_in_currencies(dictionary):\n",
    "    \"\"\"\n",
    "    This function splits the training and test results by currencies for each labeling method.\n",
    "    \n",
    "    Parameters:\n",
    "    dictionary (dict): A dictionary where keys are labeling method names and values are dictionaries containing\n",
    "                       the trained model, training and test datasets, and true and predicted labels.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary where keys are labeling method names and values are dictionaries containing training \n",
    "          and test results split by currencies.\n",
    "    \"\"\"\n",
    "    \n",
    "    currencies = {}\n",
    "    \n",
    "    # Reverse the valid_currencies dictionary for detokenizing\n",
    "    valid_currencies2 = {y: x for x, y in valid_currencies.items()}\n",
    "    \n",
    "    # Iterate over each labeling method in the dictionary\n",
    "    for labeling_method in dictionary:\n",
    "        \n",
    "        # Initialize a nested dictionary for each labeling method\n",
    "        currencies[labeling_method] = {}\n",
    "        \n",
    "        # Copy training and test DataFrames\n",
    "        train_df = dictionary[labeling_method][\"X_train\"].copy()\n",
    "        test_df = dictionary[labeling_method][\"X_test\"].copy()\n",
    "        \n",
    "        # Add true and predicted labels to the training DataFrame\n",
    "        train_df[\"true label\"] = dictionary[labeling_method][\"y_train\"].values\n",
    "        train_df[\"predicted label\"] = dictionary[labeling_method][\"y_pred_train\"].values\n",
    "        \n",
    "        # Add true and predicted labels to the test DataFrame\n",
    "        test_df[\"true label\"] = dictionary[labeling_method][\"y_test\"].values\n",
    "        test_df[\"predicted label\"] = dictionary[labeling_method][\"y_pred_test\"].values\n",
    "        \n",
    "        # Detokenize the currencies to make them readable\n",
    "        train_df = train_df.replace({\"currency\": valid_currencies2})\n",
    "        test_df = test_df.replace({\"currency\": valid_currencies2})\n",
    "        \n",
    "        # Split the results by currencies\n",
    "        for currency in valid_currencies.keys():\n",
    "            \n",
    "            # Initialize a nested dictionary for each currency\n",
    "            currencies[labeling_method][currency] = {}\n",
    "            \n",
    "            # Filter the training and test DataFrames for the current currency\n",
    "            train_currency_df = train_df.loc[train_df['currency'] == currency]\n",
    "            test_currency_df = test_df.loc[test_df['currency'] == currency]\n",
    "            \n",
    "            # Store the filtered DataFrames in the dictionary\n",
    "            currencies[labeling_method][currency][\"train\"] = train_currency_df\n",
    "            currencies[labeling_method][currency][\"test\"] = test_currency_df\n",
    "            \n",
    "    return currencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1fb49a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, labeling_method, save_path=\"visualizations/ml_metrics\"):\n",
    "    \"\"\"\n",
    "    This function plots and saves the confusion matrix for the given true and predicted labels.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (array-like): Array of true labels.\n",
    "    y_pred (array-like): Array of predicted labels.\n",
    "    classes (list): List of class labels to display on the axes of the confusion matrix.\n",
    "    labeling_method (str): Name of the labeling method to include in the saved plot's filename.\n",
    "    save_path (str, optional): Directory where the plot will be saved. Default is \"visualizations/ml_metrics\".\n",
    "\n",
    "    Returns:\n",
    "    None: This function saves the confusion matrix plot to the specified directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=classes)\n",
    "\n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.set(font_scale=1.2)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=classes, yticklabels=classes)\n",
    "\n",
    "    # Add titles and labels\n",
    "    plt.title('Confusion Matrix - Actuals and Predicted')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    # Save the plot\n",
    "    save_name = os.path.join(save_path, f\"confusion_matrix_{labeling_method}.png\")\n",
    "    plt.savefig(save_name)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ac8e3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_visualize_model(model_dict, labeling_method, save_path=\"visualizations/ml_metrics\"):\n",
    "    \"\"\"\n",
    "    This function evaluates the performance of the model by generating and saving a confusion matrix,\n",
    "    and printing a classification report for the test dataset.\n",
    "\n",
    "    Parameters:\n",
    "    model_dict (dict): A dictionary containing the trained model, test dataset, and predicted labels.\n",
    "    labeling_method (str): The name of the labeling method to include in the saved plot's filename.\n",
    "    save_path (str, optional): Directory where the confusion matrix plot will be saved. Default is \"visualizations/ml_metrics\".\n",
    "\n",
    "    Returns:\n",
    "    None: This function saves the confusion matrix plot and prints the classification report.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract true and predicted labels for the test dataset\n",
    "    y_test = model_dict[\"y_test\"]\n",
    "    y_pred_test = model_dict[\"y_pred_test\"]\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    y_test = y_test.to_numpy()\n",
    "    y_pred_test = y_pred_test.to_numpy()\n",
    "    \n",
    "    # Ensure y_test and y_pred_test have the same dimensions\n",
    "    if y_test.ndim > 1:\n",
    "        y_test = y_test.ravel()\n",
    "    if y_pred_test.ndim > 1:\n",
    "        y_pred_test = y_pred_test.ravel()\n",
    "    \n",
    "    # Get unique classes from true and predicted labels\n",
    "    classes = np.unique(np.concatenate((y_test, y_pred_test), axis=0))\n",
    "\n",
    "    # Plot and save the confusion matrix\n",
    "    plot_confusion_matrix(y_test, y_pred_test, classes, labeling_method, save_path)\n",
    "\n",
    "    # Print classification report for the test dataset\n",
    "    print(\"Classification Report - Test Data:\")\n",
    "    print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7052d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_visualize_all_approaches(dictionary):\n",
    "    \"\"\"\n",
    "    Evaluate and visualize the performance of models for all labeling methods.\n",
    "    \n",
    "    Parameters:\n",
    "    dictionary (dict): A dictionary where keys are labeling method names and values are dictionaries containing\n",
    "                       the test dataset and predicted labels.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the classification report metrics for all labeling methods.\n",
    "    \"\"\"\n",
    "\n",
    "    # Mapping of internal names to real names\n",
    "    labeling_method_names = {\n",
    "        'excess_over_mean': 'Excess over Mean',\n",
    "        'excess_over_median': 'Excess over Median',\n",
    "        'fixed_time_horizon': 'Fixed Time Horizon',\n",
    "        'triple_barrier': 'Triple Barrier',\n",
    "        'tail_sets': 'Tail Sets',\n",
    "        'matrix_flag': 'Matrix Flag',\n",
    "        'trend_scanning': 'Trend Scanning',\n",
    "        'buy_and_hold': 'Buy and Hold',\n",
    "        \"next_period\": 'Next Period Labeling'\n",
    "    }\n",
    "\n",
    "    all_reports_df = pd.DataFrame()\n",
    "    \n",
    "    # Iterate over each labeling method in the dictionary\n",
    "    for labeling_method in dictionary:\n",
    "        # Evaluate model performance for the current labeling method\n",
    "        report_df = evaluate_model_performance(dictionary[labeling_method], labeling_method_names.get(labeling_method, labeling_method))\n",
    "        all_reports_df = pd.concat([all_reports_df, report_df], ignore_index=True)\n",
    "    \n",
    "    return all_reports_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bda2107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_labeling_techniques(dictionary):\n",
    "    \"\"\"\n",
    "    This function evaluates various ML metrics of trained models for multiple labeling methods and stores the results in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    dictionary (dict): A dictionary where keys are labeling method names and values are dictionaries \n",
    "                       containing the trained model, training and test datasets, and true labels.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing the evaluation metrics for each labeling method.\n",
    "    \"\"\"\n",
    "\n",
    "    # Mapping of internal names to real names\n",
    "    labeling_method_names = {\n",
    "        'excess_over_mean': 'Excess over Mean',\n",
    "        'excess_over_median': 'Excess over Median',\n",
    "        'fixed_time_horizon': 'Fixed Time Horizon',\n",
    "        'triple_barrier': 'Triple Barrier',\n",
    "        'tail_sets': 'Tail Sets',\n",
    "        'matrix_flag': 'Matrix Flag',\n",
    "        'trend_scanning': 'Trend Scanning',\n",
    "        'buy_and_hold': 'Buy and Hold',\n",
    "        \"next_period\": 'Next Period Labeling'\n",
    "    }\n",
    "    \n",
    "    # Initialize an empty list to store results\n",
    "    results = []\n",
    "\n",
    "    # Iterate over each labeling method in the dictionary\n",
    "    for labeling_method, data in dictionary.items():\n",
    "        model = data['model']\n",
    "        X_train = data['X_train']\n",
    "        y_train = data['y_train']\n",
    "        X_test = data['X_test']\n",
    "        y_test = data['y_test']\n",
    "        \n",
    "        # Predict the labels for the test set\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_test_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "        test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "        balanced_acc = balanced_accuracy_score(y_test, y_test_pred)\n",
    "        \n",
    "        # Append the results to the list\n",
    "        results.append({\n",
    "            'Labeling Method': labeling_method_names.get(labeling_method, labeling_method),\n",
    "            'Train Accuracy': train_accuracy,\n",
    "            'Test Accuracy': test_accuracy,\n",
    "            'Balanced Accuracy': balanced_acc\n",
    "        })\n",
    "    \n",
    "    # Convert the results list to a DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86a27f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model_dict, labeling_method):\n",
    "    \"\"\"\n",
    "    Evaluate the performance of the model by generating a classification report.\n",
    "    \n",
    "    Parameters:\n",
    "    model_dict (dict): A dictionary containing the test dataset and predicted labels.\n",
    "    labeling_method (str): The name of the labeling method.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the classification report metrics.\n",
    "    \"\"\"\n",
    "    # Extract true and predicted labels for the test dataset\n",
    "    y_test = model_dict[\"y_test\"]\n",
    "    y_pred_test = model_dict[\"y_pred_test\"]\n",
    "    \n",
    "    # Ensure y_test and y_pred_test have the same dimensions\n",
    "    y_test = y_test.to_numpy().ravel()\n",
    "    y_pred_test = y_pred_test.to_numpy().ravel()\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred_test)\n",
    "    \n",
    "    # Convert classification report to DataFrame\n",
    "    report_df = classification_report_to_df(report, labeling_method)\n",
    "    \n",
    "    return report_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c24fe074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_report_to_df(report, method):\n",
    "    \"\"\"\n",
    "    Convert classification report to a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    report (str): The classification report as a string.\n",
    "    method (str): The labeling method name.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the classification report metrics.\n",
    "    \"\"\"\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        if line.strip() == \"\":\n",
    "            continue\n",
    "        row_data = line.split()\n",
    "        \n",
    "        if len(row_data) < 2:  # Skip empty or malformed lines\n",
    "            continue\n",
    "        \n",
    "        if row_data[0] in ['accuracy', 'macro', 'weighted']:\n",
    "            continue\n",
    "\n",
    "        if len(row_data) == 5:\n",
    "            row = {\n",
    "                'Method': method,\n",
    "                'Class': row_data[0],\n",
    "                'Precision': float(row_data[1]),\n",
    "                'Recall': float(row_data[2]),\n",
    "                'F1-Score': float(row_data[3]),\n",
    "                'Support': int(row_data[4])\n",
    "            }\n",
    "            report_data.append(row)\n",
    "    \n",
    "    df = pd.DataFrame.from_dict(report_data)\n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
